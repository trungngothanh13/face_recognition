# src/analytics/spark_analytics.py (WINDOWS SPARK FIX)
"""
Data Analytics Engine - Optimized for Windows with Hadoop support
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
import sys
import os
import tempfile
import subprocess

warnings.filterwarnings('ignore')

class SparkAnalyticsEngine:
    """Analytics engine with improved Windows Spark support"""
    
    def __init__(self, app_name="FaceRecognitionAnalytics"):
        """Initialize analytics engine with Windows optimizations"""
        self.use_spark = False
        self.spark = None
        
        try:
            print("üîß Setting up Windows Spark environment...")
            self._setup_windows_spark_environment()
            self._init_spark_with_hadoop(app_name)
        except Exception as e:
            print(f"‚ö†Ô∏è Spark failed, using pandas: {str(e)[:50]}...")
            self.use_spark = False
    
    def _setup_windows_spark_environment(self):
        """Enhanced Windows environment setup using your Hadoop installation"""
        
        # Set Python paths
        python_exe = sys.executable
        os.environ['PYSPARK_PYTHON'] = python_exe
        os.environ['PYSPARK_DRIVER_PYTHON'] = python_exe
        
        # Use your existing Hadoop installation
        hadoop_home = r"C:\hadoop-3.0.0"
        if os.path.exists(hadoop_home):
            os.environ['HADOOP_HOME'] = hadoop_home
            os.environ['HADOOP_CONF_DIR'] = os.path.join(hadoop_home, 'etc', 'hadoop')
            
            # Add Hadoop bin to PATH for winutils
            hadoop_bin = os.path.join(hadoop_home, 'bin')
            current_path = os.environ.get('PATH', '')
            if hadoop_bin not in current_path:
                os.environ['PATH'] = f"{hadoop_bin};{current_path}"
            
            print(f"‚úÖ Using Hadoop: {hadoop_home}")
        else:
            print("‚ö†Ô∏è Hadoop not found at expected location")
        
        # Java setup - you have jdk-11
        java_home = r"C:\Program Files\Java\jdk-11"
        if os.path.exists(java_home):
            os.environ['JAVA_HOME'] = java_home
            java_bin = os.path.join(java_home, 'bin')
            current_path = os.environ.get('PATH', '')
            if java_bin not in current_path:
                os.environ['PATH'] = f"{java_bin};{current_path}"
            print(f"‚úÖ Using Java: {java_home}")
        
        # Spark specific Windows settings
        os.environ['SPARK_LOCAL_IP'] = '127.0.0.1'
        os.environ['SPARK_LOCAL_HOSTNAME'] = 'localhost'
        
        # Create temp directories
        temp_base = tempfile.gettempdir()
        spark_temp = os.path.join(temp_base, 'spark-temp')
        os.makedirs(spark_temp, exist_ok=True)
        os.environ['SPARK_LOCAL_DIRS'] = spark_temp
        os.environ['TMPDIR'] = spark_temp
        
        print("‚úÖ Windows Spark environment configured")
    
    def _init_spark_with_hadoop(self, app_name):
        """Initialize Spark with Hadoop and Windows optimizations"""
        from pyspark.sql import SparkSession
        from pyspark.conf import SparkConf
        
        # Create optimized Spark configuration for Windows
        conf = SparkConf()
        conf.set("spark.app.name", app_name)
        conf.set("spark.master", "local[1]")  # Use single thread to avoid socket issues
        conf.set("spark.driver.memory", "512m")
        conf.set("spark.executor.memory", "512m")
        conf.set("spark.driver.maxResultSize", "256m")
        
        # Windows specific configurations
        conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
        conf.set("spark.sql.adaptive.enabled", "false")  # Disable adaptive for stability
        conf.set("spark.dynamicAllocation.enabled", "false")
        
        # Network and communication settings
        conf.set("spark.driver.bindAddress", "127.0.0.1")
        conf.set("spark.driver.host", "127.0.0.1")
        conf.set("spark.blockManager.port", "0")  # Let Spark choose available ports
        conf.set("spark.driver.port", "0")
        conf.set("spark.ui.port", "0")
        
        # Reduce parallelism to minimize socket communication
        conf.set("spark.sql.shuffle.partitions", "1")
        conf.set("spark.default.parallelism", "1")
        
        # File system settings
        conf.set("spark.hadoop.fs.defaultFS", "file:///")
        conf.set("spark.sql.warehouse.dir", tempfile.gettempdir())
        
        # Create Spark session
        self.spark = SparkSession.builder \
            .config(conf=conf) \
            .getOrCreate()
        
        # Set minimal logging
        self.spark.sparkContext.setLogLevel("ERROR")
        
        # Test with minimal operation
        try:
            test_rdd = self.spark.sparkContext.parallelize([1, 2, 3], 1)
            count = test_rdd.count()
            
            if count == 3:
                self.use_spark = True
                print("‚úÖ Spark Analytics Engine initialized with Hadoop support")
            else:
                raise Exception("Spark test failed")
                
        except Exception as test_error:
            print(f"‚ö†Ô∏è Spark test failed: {test_error}")
            raise test_error
    
    def load_data_from_mongodb(self):
        """Load data from MongoDB"""
        try:
            from ..database.database_manager import get_database_manager
            
            db_manager = get_database_manager()
            events_collection = db_manager.get_collection("recognition_events")
            attendance_collection = db_manager.get_collection("attendance")
            
            events_data = list(events_collection.find())
            attendance_data = list(attendance_collection.find())
            
            # Process events
            if events_data:
                for event in events_data:
                    if '_id' in event:
                        del event['_id']
                
                self.events_df = pd.DataFrame(events_data)
                if 'timestamp' in self.events_df.columns:
                    self.events_df['timestamp'] = pd.to_datetime(self.events_df['timestamp'])
                    self.events_df['hour'] = self.events_df['timestamp'].dt.hour
                    self.events_df['day_of_week'] = self.events_df['timestamp'].dt.day_name()
            else:
                self.events_df = pd.DataFrame()
            
            # Process attendance
            if attendance_data:
                for attendance in attendance_data:
                    if '_id' in attendance:
                        del attendance['_id']
                
                self.attendance_df = pd.DataFrame(attendance_data)
                if 'enter_time' in self.attendance_df.columns:
                    self.attendance_df['enter_time'] = pd.to_datetime(self.attendance_df['enter_time'])
                    self.attendance_df['day_of_week'] = self.attendance_df['enter_time'].dt.day_name()
            else:
                self.attendance_df = pd.DataFrame()
            
            # Add sample data if needed
            if len(events_data) < 50:
                print("üìä Adding sample data for demonstration...")
                self._create_sample_data()
            else:
                print(f"üìä Loaded: {len(events_data)} events, {len(attendance_data)} attendance")
            
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è Using sample data: {e}")
            self._create_sample_data()
            return False
    
    def _create_sample_data(self):
        """Create sample data"""
        print("üîß Generating sample data...")
        
        np.random.seed(42)
        employees = ["John_Doe", "Jane_Smith", "Mike_Wilson", "Sarah_Johnson", "David_Brown"]
        start_date = datetime.now() - timedelta(days=30)
        
        # Events data
        events_data = []
        for i in range(200):  # Smaller dataset for testing
            day_offset = int(np.random.randint(0, 30))
            hour = int(np.random.choice([8, 9, 17, 18]))
            event_time = start_date + timedelta(days=day_offset, hours=hour)
            
            events_data.append({
                "name": str(np.random.choice(employees)),
                "confidence": float(np.random.uniform(0.7, 0.95)),
                "timestamp": event_time,
                "hour": int(event_time.hour),
                "day_of_week": event_time.strftime("%A")
            })
        
        self.events_df = pd.DataFrame(events_data)
        
        # Attendance data
        attendance_data = []
        for i in range(50):  # Smaller dataset
            attend_date = start_date + timedelta(days=int(np.random.randint(0, 30)))
            hour = int(np.random.choice([8, 9]))
            attend_time = attend_date.replace(hour=hour)
            
            attendance_data.append({
                "employee_name": str(np.random.choice(employees)),
                "enter_time": attend_time,
                "is_late": bool(hour >= 9),
                "day_of_week": attend_time.strftime("%A")
            })
        
        self.attendance_df = pd.DataFrame(attendance_data)
        print(f"‚úÖ Generated {len(self.events_df)} events, {len(self.attendance_df)} attendance")
    
    def safe_format_number(self, value, format_type="int"):
        """Safely format numbers to avoid errors"""
        try:
            if value is None:
                return "N/A"
            
            if format_type == "int":
                return f"{int(float(value)):,}"
            elif format_type == "float":
                return f"{float(value):.3f}"
            elif format_type == "percent":
                return f"{float(value):.1f}%"
            else:
                return str(value)
        except (ValueError, TypeError):
            return "N/A"
    
    def analyze_peak_hours(self):
        """Analyze peak hours with optimized Spark operations"""
        print("üìà Analyzing peak hours...")
        
        if self.events_df.empty:
            return pd.DataFrame()
        
        try:
            if self.use_spark and self.spark:
                try:
                    print("üî• Using Spark for peak hours analysis...")
                    
                    # Create DataFrame with minimal data
                    spark_data = self.events_df[['hour', 'name', 'confidence']].copy()
                    
                    # Create Spark DataFrame with single partition
                    spark_df = self.spark.createDataFrame(spark_data).coalesce(1)
                    
                    from pyspark.sql.functions import count, avg, countDistinct
                    
                    # Simple aggregation to minimize shuffling
                    result = spark_df.groupBy("hour").agg(
                        count("*").alias("recognition_count"),
                        avg("confidence").alias("avg_confidence"),
                        countDistinct("name").alias("unique_people")
                    ).orderBy("hour")
                    
                    # Collect results immediately
                    pandas_result = result.toPandas()
                    
                    # Fix data types
                    pandas_result['recognition_count'] = pandas_result['recognition_count'].astype(int)
                    pandas_result['avg_confidence'] = pandas_result['avg_confidence'].astype(float).round(3)
                    pandas_result['unique_people'] = pandas_result['unique_people'].astype(int)
                    
                    print("‚úÖ Spark peak hours analysis completed")
                    return pandas_result
                    
                except Exception as spark_error:
                    print(f"‚ö†Ô∏è Spark analysis failed: {spark_error}")
                    self.use_spark = False
            
            # Pandas fallback
            print("üêº Using pandas for peak hours analysis...")
            result = self.events_df.groupby('hour').agg({
                'name': ['count', 'nunique'],
                'confidence': 'mean'
            }).round(3)
            
            result.columns = ['recognition_count', 'unique_people', 'avg_confidence']
            result = result.reset_index()
            
            # Fix data types
            result['hour'] = result['hour'].astype(int)
            result['recognition_count'] = result['recognition_count'].astype(int)
            result['unique_people'] = result['unique_people'].astype(int)
            result['avg_confidence'] = result['avg_confidence'].astype(float).round(3)
            
            return result
            
        except Exception as e:
            print(f"Error in peak hours: {e}")
            return pd.DataFrame()
    
    def analyze_daily_patterns(self):
        """Analyze daily patterns"""
        print("üìÖ Analyzing daily patterns...")
        
        if self.attendance_df.empty:
            return pd.DataFrame()
        
        try:
            daily_stats = self.attendance_df.groupby('day_of_week').agg({
                'employee_name': ['count', 'nunique'],
                'is_late': 'sum'
            })
            
            daily_stats.columns = ['total_attendance', 'unique_employees', 'late_count']
            daily_stats['late_percentage'] = (daily_stats['late_count'] / daily_stats['total_attendance'] * 100).round(2)
            
            # Fix data types
            daily_stats['total_attendance'] = daily_stats['total_attendance'].astype(int)
            daily_stats['unique_employees'] = daily_stats['unique_employees'].astype(int)
            daily_stats['late_count'] = daily_stats['late_count'].astype(int)
            daily_stats['late_percentage'] = daily_stats['late_percentage'].astype(float)
            
            return daily_stats.reset_index()
            
        except Exception as e:
            print(f"Error in daily patterns: {e}")
            return pd.DataFrame()
    
    def analyze_employee_performance(self):
        """Analyze employee performance"""
        print("üë• Analyzing employee performance...")
        
        if self.attendance_df.empty:
            return pd.DataFrame()
        
        try:
            emp_stats = self.attendance_df.groupby('employee_name').agg({
                'employee_name': 'count',
                'is_late': 'sum',
                'enter_time': lambda x: x.dt.hour.mean()
            }).round(2)
            
            emp_stats.columns = ['total_days', 'late_days', 'avg_arrival_hour']
            emp_stats['punctuality_score'] = ((emp_stats['total_days'] - emp_stats['late_days']) / emp_stats['total_days'] * 100).round(2)
            
            # Fix data types
            emp_stats['total_days'] = emp_stats['total_days'].astype(int)
            emp_stats['late_days'] = emp_stats['late_days'].astype(int)
            emp_stats['avg_arrival_hour'] = emp_stats['avg_arrival_hour'].astype(float).round(1)
            emp_stats['punctuality_score'] = emp_stats['punctuality_score'].astype(float)
            
            return emp_stats.sort_values('punctuality_score', ascending=False).reset_index()
            
        except Exception as e:
            print(f"Error in employee performance: {e}")
            return pd.DataFrame()
    
    def analyze_recognition_accuracy_trends(self):
        """Analyze accuracy trends"""
        print("üéØ Analyzing accuracy trends...")
        
        if self.events_df.empty:
            return pd.DataFrame()
        
        try:
            self.events_df['week'] = self.events_df['timestamp'].dt.isocalendar().week
            
            weekly_stats = self.events_df.groupby('week').agg({
                'name': 'count',
                'confidence': ['mean', 'min', 'max', 'std']
            }).round(3)
            
            weekly_stats.columns = ['total_recognitions', 'avg_confidence', 'min_confidence', 'max_confidence', 'confidence_std']
            weekly_stats = weekly_stats.fillna(0)
            
            # Fix data types
            weekly_stats['total_recognitions'] = weekly_stats['total_recognitions'].astype(int)
            for col in ['avg_confidence', 'min_confidence', 'max_confidence', 'confidence_std']:
                weekly_stats[col] = weekly_stats[col].astype(float).round(3)
            
            return weekly_stats.reset_index()
            
        except Exception as e:
            print(f"Error in accuracy trends: {e}")
            return pd.DataFrame()
    
    def real_time_analytics_simulation(self):
        """Real-time analytics with date column and sorted by date"""
        print("‚ö° Running real-time analytics...")

        if self.events_df.empty:
            return pd.DataFrame()

        try:
            recent_cutoff = datetime.now() - timedelta(days=7)
            recent_events = self.events_df[self.events_df['timestamp'] >= recent_cutoff]

            if recent_events.empty:
                recent_events = self.events_df.tail(100)

            # Add a 'date' column (date only, not datetime)
            recent_events = recent_events.copy()
            recent_events['date'] = recent_events['timestamp'].dt.date

            realtime_stats = recent_events.groupby(['date', 'name', 'day_of_week']).agg({
                'name': 'count',
                'confidence': 'mean'
            }).round(3)

            realtime_stats.columns = ['recognition_frequency', 'avg_confidence']

            # Fix data types
            realtime_stats['recognition_frequency'] = realtime_stats['recognition_frequency'].astype(int)
            realtime_stats['avg_confidence'] = realtime_stats['avg_confidence'].astype(float).round(3)

            # Sort by date (ascending)
            return realtime_stats.sort_values('date', ascending=True).reset_index()

        except Exception as e:
            print(f"Error in real-time analytics: {e}")
            return pd.DataFrame()
    
    def generate_comprehensive_report(self):
        """Generate comprehensive report"""
        print("üìä Generating comprehensive report...")
        
        try:
            report = {
                "peak_hours": self.analyze_peak_hours(),
                "daily_patterns": self.analyze_daily_patterns(),
                "employee_performance": self.analyze_employee_performance(),
                "accuracy_trends": self.analyze_recognition_accuracy_trends(),
                "real_time_insights": self.real_time_analytics_simulation()
            }
            
            # Performance metrics
            events_count = len(self.events_df) if hasattr(self, 'events_df') and not self.events_df.empty else 0
            attendance_count = len(self.attendance_df) if hasattr(self, 'attendance_df') and not self.attendance_df.empty else 0
            
            processing_mode = "Apache Spark with Hadoop Support" if self.use_spark else "Pandas High-Performance Processing"
            
            report["performance_metrics"] = {
                "total_events_processed": events_count,
                "total_attendance_records": attendance_count,
                "processing_time": processing_mode,
                "data_size_gb": f"~{(events_count + attendance_count) * 0.001:.1f}GB estimated",
                "cluster_utilization": "Hadoop-optimized processing" if self.use_spark else "Single-node optimized"
            }
            
            print("‚úÖ Analytics report generated!")
            return report
            
        except Exception as e:
            print(f"‚ùå Error generating report: {e}")
            return {
                "error": str(e),
                "performance_metrics": {
                    "total_events_processed": 0,
                    "total_attendance_records": 0,
                    "processing_time": "Error occurred",
                    "data_size_gb": "Unknown",
                    "cluster_utilization": "Error"
                }
            }
    
    def close(self):
        """Close Spark session"""
        if self.spark:
            try:
                self.spark.stop()
                print("üì¥ Spark Analytics Engine stopped")
            except Exception:
                pass
            self.spark = None